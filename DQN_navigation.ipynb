{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banana Agent Project\n",
    "---\n",
    "\n",
    "This notebook is for training an agent using a deep Q network algorithm to solve the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Setting up the Environment\n",
    "----\n",
    "\n",
    "Firstly we will set up the Unity banana collection environment. Please make sure you are running a Python 3.6 kernel for this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if not (sys.version_info[0] == 3 and sys.version_info[1] == 6):\n",
    "    raise Exception(\"This notebook must be run with Python 3.6 to allow unityagents package to work correctly.\")\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from dqn_banana_agent import DQNAgent, train_agent, load_agent, test_agent, criterion_check\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the environment please download the correct Unity environment for your OS using the following links. \n",
    "\n",
    "- Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip)\n",
    "- Linux (Headless): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux_NoVis.zip)\n",
    "- Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip)\n",
    "- Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86.zip)\n",
    "- Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip)\n",
    "\n",
    "Once downloaded change the `path` variable to match the location of the environment that you downloaded.\n",
    "\n",
    "<strong>Note: only run the following cell once. If the Unity environment has been closed or crashed please restart the kernel. <strong/>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Banana.app\"\n",
    "env = UnityEnvironment(file_name=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training Model\n",
    "-----\n",
    "\n",
    "In this section you are able to train a model from scratch using different hyperparameters and neural architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting parameter infomation from the Unity environment\n",
    "brain_name = env.brain_names[0]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "brain = env.brains[brain_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells allows you to define hyperparameters and create your model.\n",
    "\n",
    "Note: The numbers commented next to each parameter were what I used to solve the banana collection problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent hyperparameters\n",
    "buffer_size=20000 #20000\n",
    "seed=10 #10\n",
    "\n",
    "state_size=env_info.vector_observations.shape[1]\n",
    "action_size=brain.vector_action_space_size\n",
    "hidden_layers=[64,64,32] #[64,64,32]\n",
    "\n",
    "epsilon=1. #1.\n",
    "epsilon_decay=0.99995 #0.99995\n",
    "epsilon_min=0.02 #0.02\n",
    "\n",
    "gamma=0.95 #0.95\n",
    "tau=0.001 #0.001\n",
    "learning_rate=0.001 #0.001\n",
    "update_frequency=5 #5\n",
    "\n",
    "double_Q=True #True\n",
    "\n",
    "prioritised_replay_buffer=True #True\n",
    "alpha=0.6 #0.6\n",
    "beta=0.7 #0.7\n",
    "beta_increment_size=0.00001 #0.00001\n",
    "base_priority=0.1 #0.1\n",
    "max_priority=1 #1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates agent with above hyperparameters.\n",
    " agent = DQNAgent(\n",
    "        buffer_size,\n",
    "        seed,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        hidden_layers,\n",
    "        epsilon,\n",
    "        epsilon_decay,\n",
    "        epsilon_min,\n",
    "        gamma,\n",
    "        tau,\n",
    "        learning_rate,\n",
    "        update_frequency,\n",
    "        double_Q,\n",
    "        prioritised_replay_buffer,\n",
    "        alpha,\n",
    "        beta,\n",
    "        beta_increment_size,\n",
    "        base_priority,\n",
    "        max_priority\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally this cell allows to train a new or loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commence training proccess\n",
    "no_training_episodes = 100\n",
    "experiance_batch_size = 128\n",
    "save_name = \"new_dqn_model.pth\" # choose a name for the model (eg DQNagent.pth)\n",
    "save_path = \"\"  # choose a path to save the best and final models, leave blank for current dir.\n",
    "print_every = 25\n",
    "\n",
    "scores = train_agent(env, agent, no_training_episodes, experiance_batch_size, save_name, save_path, print_every)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loading Saved Model\n",
    "----\n",
    "\n",
    "Use this section to reload a previously saved model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_path = \"Trained_agents/double_per_tuned.pth\" # Trained_agents/double_per_tuned.pth is an agent trained to solve the enviroment\n",
    "                                                   # using a double Q and PER. \n",
    "show_parameters = True\n",
    "agent = load_agent(agent_path, show_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Testing\n",
    "----\n",
    "\n",
    "The next cell will run the agent in evaluation mode and save the episode scores. If you would like to watch the agents actions set `quick_view` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episode_number = 100\n",
    "print_scores = True\n",
    "quick_view = True\n",
    "\n",
    "test_scores = test_agent(env, agent, test_episode_number, print_scores, quick_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Evaluation\n",
    "----\n",
    "In this section we can create graphs to evaluate the models training process and how it preforms during training as well as checking if the environment has been solved.\n",
    "The following cell closes the unity environment, converts the agent's training scores into a Panda series for easy of analysis and increase plot size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()\n",
    "train_scores = pd.Series(agent.training_scores)\n",
    "plt.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell plots both episode score and a rolling average for training rewards of the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Training Rewards per Episode\")\n",
    "plt.plot(train_scores,label = \"Individual episode reward\",color=\"midnightblue\")\n",
    "plt.plot(train_scores.rolling(50).mean(), label = \"50 episode moving average\", color=\"r\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Total Episode Reward\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final cell calculates the episode on which the >=13 reward over 100 episodes criteria was achieved and plots a graphs avergaed over  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passed = criterion_check(agent)\n",
    "plt.title(\"Average Score over 100 Consecutive Training Episodes\")\n",
    "plt.plot(train_scores.rolling(100).mean()[100:], label=\"100 episode moving average\", color=\"midnightblue\")\n",
    "plt.hlines(13,100,len(train_scores),label=\">13 threshold\",linestyle=\"--\",color=\"r\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Total Episode Reward\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
